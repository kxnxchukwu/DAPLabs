{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAP Lab 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Create a text file and manually add some data to the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to illustrate \n",
    "# Append vs write mode \n",
    "file = open(\"myfile.txt\",\"w\") \n",
    "L = \"This is Lagos\\nThis is Abuja\\nThis is Benin\\n\"\n",
    "file.write(L)\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Write Python code to open the file for write only access and attempt to read the contents of the file \n",
    "\n",
    "### 3) Note the type of Error that has been raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "not readable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2f3c5d080c15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"myfile.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: not readable"
     ]
    }
   ],
   "source": [
    "file = open(\"myfile.txt\",\"w\") \n",
    "file.readlines()\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Modify your code to use a try / except / finally construct that will catch the exception, print a userfriendly error message, and clean up the file resource "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not readable\n",
      "Finally Block\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "try:\n",
    "    file = open(\"myfile.txt\",\"w\") \n",
    "    file.readlines()\n",
    "    file.close()\n",
    "except io.UnsupportedOperation as err:\n",
    "    print(err)\n",
    "finally:\n",
    "    print(\"Finally Block\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Investigate how you would create your own Exception class. Then create your own Exception class and use it in your code from the previous exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomException(Exception):\n",
    "     def __init__(self):\n",
    "         default_message = 'This is a default message!'\n",
    "         super().__init__(default_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Except Block\n"
     ]
    },
    {
     "ename": "CustomException",
     "evalue": "This is a default message!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCustomException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-3eb70c5f9175>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Except Block'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mCustomException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCustomException\u001b[0m: This is a default message!"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    file = open(\"myfile.txt\",\"w\") \n",
    "    file.readlines()\n",
    "    file.close()\n",
    "except:\n",
    "    print('Except Block')\n",
    "finally:\n",
    "    raise CustomException()\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Create an array with the arange function and reshape the array as follows: b = arange(24).reshape(2,3,4) This gives us a 3-dimensional data structure – you can think of it as being like 2 spreadsheet sheets where each sheet contains 3 rows of data and each row contains 4 columns. Using indexing and slicing perform the following tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "b = np.arange(24).reshape(2,3,4)\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Choose the first set of 3 rows and 4 columns of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n"
     ]
    }
   ],
   "source": [
    "print(b[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Choose the second row of data from the second set of 3 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "print(b[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Choose all the data from the second column for both the first and second sets of rows and columns of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  5  9]\n",
      " [13 17 21]]\n"
     ]
    }
   ],
   "source": [
    "print(b[:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Use the ravel function to flatten the data. What’s the difference between ravel and flatten? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n"
     ]
    }
   ],
   "source": [
    "ravelOutput = np.ravel(b)\n",
    "\n",
    "print(ravelOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n"
     ]
    }
   ],
   "source": [
    "flattenOutput = b.flatten()\n",
    "\n",
    "print(flattenOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Reshape the data so that there are 6 rows of 4 columns per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]\n",
      " [16 17 18 19]\n",
      " [20 21 22 23]]\n"
     ]
    }
   ],
   "source": [
    "reshapeOutput = b.reshape(6,4)\n",
    "\n",
    "print(reshapeOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Get the transpose of the new data structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0 12]\n",
      "  [ 4 16]\n",
      "  [ 8 20]]\n",
      "\n",
      " [[ 1 13]\n",
      "  [ 5 17]\n",
      "  [ 9 21]]\n",
      "\n",
      " [[ 2 14]\n",
      "  [ 6 18]\n",
      "  [10 22]]\n",
      "\n",
      " [[ 3 15]\n",
      "  [ 7 19]\n",
      "  [11 23]]]\n"
     ]
    }
   ],
   "source": [
    "transposedOutput = b.T\n",
    "\n",
    "print(transposedOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Restack the rows of the transposed data structure in reverse order (hint: look at the row_stack function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3 12 13 14 15]\n",
      " [ 4  5  6  7 16 17 18 19]\n",
      " [ 8  9 10 11 20 21 22 23]]\n"
     ]
    }
   ],
   "source": [
    "StackedRows = np.row_stack(transposedOutput[::-1])\n",
    "\n",
    "print(StackedRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Split the resulting data structure horizontally (hint: look at the hsplit function). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[ 0,  1,  2,  3]],\n",
      "\n",
      "       [[12, 13, 14, 15]]]), array([[[ 4,  5,  6,  7]],\n",
      "\n",
      "       [[16, 17, 18, 19]]]), array([[[ 8,  9, 10, 11]],\n",
      "\n",
      "       [[20, 21, 22, 23]]])]\n"
     ]
    }
   ],
   "source": [
    "SplitArray = np.hsplit(b, 3)\n",
    "\n",
    "print(SplitArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Exercise 2\n",
    "\n",
    "###  NOTE: The AAPL.csv contains some stock price data for Apple.           The MSFT.csv contains some stock price data for Microsoft. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Use the loadtxt command to load data from AAPL.csv from columns 5 and 7 (i.e., the close price and the volume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([223.770004, 226.869995, 216.360001, 214.449997, 222.110001,\n",
      "       217.360001, 222.149994, 221.190002, 216.020004, 219.309998,\n",
      "       220.649994, 222.729996, 215.089996, 219.800003, 216.300003,\n",
      "       212.240005, 213.300003, 218.860001, 222.220001, 207.479996,\n",
      "       201.589996, 203.770004, 209.949997, 208.490005, 204.470001,\n",
      "       194.169998, 192.229996, 186.800003, 191.410004, 193.529999,\n",
      "       185.860001, 176.979996, 176.779999, 172.289993, 174.619995,\n",
      "       174.240005, 180.940002, 179.550003, 178.580002, 184.820007,\n",
      "       176.690002, 174.720001, 168.490005, 169.600006, 168.630005,\n",
      "       169.100006, 170.949997, 165.479996, 163.940002, 166.070007,\n",
      "       160.889999, 156.830002, 150.729996, 146.830002, 157.169998,\n",
      "       156.149994, 156.229996, 157.740005, 157.919998, 142.190002,\n",
      "       148.259995, 147.929993, 150.75    , 153.309998, 153.800003,\n",
      "       152.289993, 150.      , 153.070007, 154.940002, 155.860001,\n",
      "       156.820007, 153.300003, 153.919998, 152.699997, 157.759995,\n",
      "       156.300003, 154.679993, 165.25    , 166.440002, 166.520004,\n",
      "       171.25    , 174.179993, 174.240005, 170.940002, 170.410004,\n",
      "       169.429993, 170.889999, 170.179993, 170.800003, 170.419998,\n",
      "       170.929993, 172.029999, 171.059998, 172.970001, 174.229996,\n",
      "       174.330002, 174.869995, 173.149994, 174.970001, 175.850006,\n",
      "       175.529999, 174.520004, 172.5     , 172.910004, 178.899994,\n",
      "       180.910004, 181.710007, 183.729996, 186.119995, 188.020004,\n",
      "       186.529999, 188.160004, 195.089996, 191.050003, 188.740005,\n",
      "       186.789993, 188.470001, 188.720001, 189.949997, 191.240005,\n",
      "       194.020004, 195.350006, 195.690002, 197.      , 200.100006,\n",
      "       199.5     , 200.619995, 198.949997, 198.869995, 199.229996,\n",
      "       199.25    , 203.130005, 203.860001, 204.529999, 207.479996,\n",
      "       207.160004, 205.279999, 204.300003, 204.610001, 200.669998,\n",
      "       210.520004, 209.149994, 211.75    , 208.479996, 202.860001,\n",
      "       202.899994, 200.720001, 197.179993, 185.720001, 188.660004,\n",
      "       190.919998, 190.080002, 189.      , 183.089996, 186.600006,\n",
      "       182.779999, 179.660004, 178.970001, 178.229996, 177.380005,\n",
      "       178.300003, 175.070007, 173.300003, 179.639999, 182.539993,\n",
      "       185.220001, 190.149994, 192.580002, 194.809998, 194.190002,\n",
      "       194.149994, 192.740005, 193.889999, 198.449997, 197.869995,\n",
      "       199.460007, 198.779999, 198.580002, 195.570007, 199.800003,\n",
      "       199.740005, 197.919998, 201.550003, 202.729996, 204.410004,\n",
      "       204.229996, 200.020004, 201.240005, 203.229996, 201.75    ,\n",
      "       203.300003, 205.210007, 204.5     , 203.350006, 205.660004,\n",
      "       202.589996, 207.220001, 208.839996, 208.669998, 207.020004,\n",
      "       207.740005, 209.679993, 208.779999, 213.039993, 208.429993,\n",
      "       204.020004, 193.339996, 197.      , 199.039993, 203.429993,\n",
      "       200.990005, 200.479996, 208.970001, 202.75    , 201.740005,\n",
      "       206.5     , 210.350006, 210.360001, 212.639999, 212.460007,\n",
      "       202.639999, 206.490005, 204.160004, 205.529999, 209.009995,\n",
      "       208.740005, 205.699997, 209.190002, 213.279999, 213.259995,\n",
      "       214.169998, 216.699997, 223.589996, 223.089996, 218.75    ,\n",
      "       219.899994, 220.699997, 222.770004, 220.960007, 217.729996,\n",
      "       218.720001, 217.679993, 221.029999, 219.889999, 218.820007,\n",
      "       223.970001, 224.589996, 218.960007, 220.820007, 227.009995,\n",
      "       227.059998, 225.809998]), array([29663900, 26891000, 41990600, 53124400, 40337900, 30791000,\n",
      "       29184000, 22885400, 32581300, 33078700, 28792100, 38767800,\n",
      "       40925500, 29855800, 47258400, 45935500, 36660000, 38358900,\n",
      "       58323200, 91328700, 66163700, 31882900, 33424400, 25362600,\n",
      "       34365800, 51135500, 46882900, 60801000, 46478800, 36928300,\n",
      "       41925300, 67825200, 31124200, 23624000, 44998500, 41387400,\n",
      "       46062500, 41770000, 39531500, 40802500, 41344300, 43098400,\n",
      "       42281600, 62026000, 47281700, 35627700, 31898600, 40703700,\n",
      "       44287900, 33841500, 49047300, 64773000, 95744600, 37169200,\n",
      "       58582500, 53117100, 42291400, 35003500, 37039700, 91312200,\n",
      "       58607100, 54777800, 41025300, 45099100, 35780700, 27023200,\n",
      "       32439200, 28710900, 30569700, 29821200, 33751000, 30394000,\n",
      "       23130600, 25441500, 33535500, 26192100, 41587200, 61109800,\n",
      "       40739600, 32668100, 31495500, 36101600, 28239600, 31741700,\n",
      "       23820000, 20993400, 22283500, 22490200, 21835700, 24626800,\n",
      "       18972800, 26114400, 17249700, 18913200, 21873400, 17070200,\n",
      "       27835400, 28215400, 25886200, 27436200, 19737400, 20810400,\n",
      "       24796400, 23999400, 32011000, 32467600, 31032500, 23579500,\n",
      "       39042900, 26219800, 31646400, 31035200, 51034200, 42407700,\n",
      "       43845300, 49800500, 29848400, 20780400, 23564000, 27862000,\n",
      "       22765700, 23271800, 19114300, 18526600, 25881700, 35768200,\n",
      "       21695300, 20900800, 27760700, 17536600, 25696400, 28906800,\n",
      "       24195800, 19439500, 23323000, 17540600, 18543200, 18649100,\n",
      "       22204700, 46534900, 64827300, 31996300, 20892400, 32443100,\n",
      "       38763700, 26339500, 34908600, 41208700, 57430600, 36529700,\n",
      "       26544700, 33031400, 32879100, 38612300, 28364800, 29748600,\n",
      "       36529700, 23714700, 27948200, 28481200, 21218400, 27043600,\n",
      "       40396100, 30968000, 29773400, 22526300, 30684400, 26220900,\n",
      "       26932900, 18253200, 21674600, 18761500, 14669100, 26551000,\n",
      "       21124200, 21514000, 47800600, 18220400, 21070300, 26067500,\n",
      "       20899700, 31110600, 27253000, 16935200, 11362000, 17265500,\n",
      "       25338600, 20578000, 17897100, 20191800, 17595200, 16947400,\n",
      "       16866800, 14107500, 18540600, 20929300, 22277900, 18355200,\n",
      "       14991600, 13909600, 17618900, 21673400, 33935700, 69281400,\n",
      "       54017900, 40862100, 52393000, 35824800, 33364400, 27009500,\n",
      "       24619700, 22481900, 47218500, 36547400, 27227400, 27620400,\n",
      "       24413600, 26884300, 21535400, 22253700, 46818000, 26043600,\n",
      "       25873300, 15938800, 20990500, 21143400, 20023000, 19188100,\n",
      "       23913700, 19362300, 27309400, 31777900, 44289600, 32226700,\n",
      "       39763300, 21158100, 18318700, 25340000, 22060600, 55413100,\n",
      "       19165500, 31190800, 21903400, 18833500, 25352000, 25977400,\n",
      "       34805800, 34612300, 28606500, 34619700, 30576500,  6153939])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "columnFiveSeven = np.loadtxt('AAPL.csv', delimiter=',', dtype='float,int', usecols=(4, 6), skiprows=1, unpack=True)\n",
    "\n",
    "print(columnFiveSeven)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Based on the data provided, calculate the volume weighted average price for the stock (i.e., calculate the average price using the volume as weight values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Weighted Price => 190.02952578964312\n"
     ]
    }
   ],
   "source": [
    "average_weighted_price = np.average(columnFiveSeven[0], axis= 0, weights= columnFiveSeven[1])\n",
    "print(f\"Average Weighted Price => {average_weighted_price}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Calculate the median value of the closing prices (hint: use the median function). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Close Price => 197.0\n"
     ]
    }
   ],
   "source": [
    "median_close_price = np.median(columnFiveSeven[0])\n",
    "print(f\"Median Close Price => {median_close_price}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Calculate the variance value of the closing prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Close Price => 432.8804568188714\n"
     ]
    }
   ],
   "source": [
    "variance_close_price = np.var(columnFiveSeven[0])\n",
    "print(f\"Variance Close Price => {variance_close_price}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Again, use the loadtxt command to load data from columns 3 and 4 (i.e., the high prices and the low prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([224.800003, 227.270004, 226.350006, 219.5     , 222.880005,\n",
      "       221.830002, 222.990005, 222.639999, 219.740005, 221.259995,\n",
      "       223.360001, 223.25    , 224.229996, 221.380005, 220.190002,\n",
      "       219.690002, 215.179993, 220.449997, 222.360001, 213.649994,\n",
      "       204.389999, 204.720001, 210.059998, 210.119995, 206.009995,\n",
      "       199.850006, 197.179993, 194.479996, 191.970001, 194.970001,\n",
      "       190.699997, 181.470001, 180.270004, 176.600006, 174.949997,\n",
      "       174.770004, 181.289993, 182.800003, 180.330002, 184.940002,\n",
      "       182.389999, 174.779999, 174.490005, 170.089996, 171.789993,\n",
      "       171.919998, 172.570007, 169.080002, 168.350006, 167.529999,\n",
      "       167.449997, 162.110001, 158.160004, 151.550003, 157.229996,\n",
      "       156.770004, 158.520004, 159.360001, 158.850006, 145.720001,\n",
      "       148.550003, 148.830002, 151.820007, 154.529999, 153.970001,\n",
      "       153.699997, 151.270004, 153.389999, 155.880005, 157.660004,\n",
      "       157.880005, 156.729996, 155.139999, 154.479996, 158.130005,\n",
      "       156.330002, 158.130005, 166.149994, 169.      , 168.979996,\n",
      "       171.660004, 175.080002, 175.570007, 173.940002, 170.660004,\n",
      "       171.210007, 171.      , 172.479996, 171.259995, 171.699997,\n",
      "       171.440002, 173.320007, 172.369995, 173.      , 175.869995,\n",
      "       175.300003, 175.      , 174.910004, 175.149994, 177.75    ,\n",
      "       176.      , 175.490005, 174.440002, 173.070007, 179.119995,\n",
      "       182.669998, 183.300003, 184.100006, 187.330002, 188.389999,\n",
      "       188.990005, 189.490005, 196.330002, 197.690002, 191.979996,\n",
      "       192.880005, 189.759995, 189.559998, 190.080002, 191.679993,\n",
      "       194.460007, 196.5     , 196.369995, 197.100006, 200.229996,\n",
      "       202.850006, 200.740005, 201.      , 200.139999, 199.850006,\n",
      "       201.369995, 203.380005, 204.149994, 204.940002, 207.75    ,\n",
      "       208.479996, 207.759995, 205.      , 205.970001, 203.399994,\n",
      "       215.309998, 212.649994, 211.839996, 208.839996, 207.419998,\n",
      "       205.339996, 201.679993, 198.850006, 189.479996, 189.699997,\n",
      "       191.75    , 192.470001, 190.899994, 184.350006, 188.      ,\n",
      "       185.710007, 180.539993, 182.139999, 180.589996, 179.350006,\n",
      "       179.229996, 177.990005, 177.919998, 179.830002, 184.990005,\n",
      "       185.470001, 191.919998, 195.369995, 196.      , 195.970001,\n",
      "       196.789993, 193.589996, 194.960007, 200.289993, 199.880005,\n",
      "       200.610001, 200.850006, 200.160004, 199.259995, 200.990005,\n",
      "       201.570007, 199.5     , 204.490005, 203.130005, 204.440002,\n",
      "       205.080002, 201.399994, 201.509995, 203.729996, 204.389999,\n",
      "       204.      , 205.869995, 206.110001, 205.089996, 205.880005,\n",
      "       206.5     , 207.229996, 208.910004, 209.149994, 209.240005,\n",
      "       209.729996, 210.639999, 210.160004, 221.369995, 218.029999,\n",
      "       206.429993, 198.649994, 198.070007, 199.559998, 203.529999,\n",
      "       202.759995, 202.050003, 212.139999, 206.440002, 205.139999,\n",
      "       207.160004, 212.729996, 213.350006, 213.649994, 214.440002,\n",
      "       212.050003, 207.190002, 208.550003, 205.720001, 209.320007,\n",
      "       210.449997, 206.979996, 209.479996, 213.970001, 214.419998,\n",
      "       216.440002, 216.779999, 223.710007, 226.419998, 220.789993,\n",
      "       220.130005, 220.820007, 222.850006, 223.759995, 222.559998,\n",
      "       219.839996, 222.490005, 221.5     , 220.940002, 220.960007,\n",
      "       224.580002, 228.220001, 223.580002, 220.960007, 227.490005,\n",
      "       229.929993, 227.199997]), array([220.199997, 222.25    , 216.050003, 212.320007, 216.839996,\n",
      "       217.270004, 216.759995, 219.339996, 213.      , 217.429993,\n",
      "       218.940002, 214.699997, 214.539993, 216.75    , 212.669998,\n",
      "       206.089996, 209.270004, 216.619995, 216.809998, 205.429993,\n",
      "       198.169998, 201.690002, 204.130005, 206.75    , 202.25    ,\n",
      "       193.789993, 191.449997, 185.929993, 186.899994, 189.460007,\n",
      "       184.990005, 175.509995, 176.550003, 172.100006, 170.259995,\n",
      "       170.880005, 174.929993, 177.699997, 177.029999, 181.210007,\n",
      "       176.270004, 170.419998, 168.300003, 163.330002, 167.      ,\n",
      "       169.020004, 169.550003, 165.279999, 162.729996, 164.389999,\n",
      "       159.089996, 155.300003, 149.630005, 146.589996, 146.720001,\n",
      "       150.070007, 154.550003, 156.479996, 154.229996, 142.      ,\n",
      "       143.800003, 145.899994, 148.520004, 149.630005, 150.860001,\n",
      "       151.509995, 149.220001, 150.050003, 153.      , 153.259995,\n",
      "       155.979996, 152.619995, 151.699997, 151.740005, 154.320007,\n",
      "       153.660004, 154.110001, 160.229996, 164.559998, 165.929993,\n",
      "       167.279999, 172.350006, 172.850006, 170.339996, 168.419998,\n",
      "       169.25    , 169.699997, 169.919998, 169.380005, 169.75    ,\n",
      "       169.490005, 170.990005, 170.300003, 171.380005, 173.949997,\n",
      "       173.169998, 172.729996, 172.919998, 172.889999, 173.970001,\n",
      "       174.539993, 173.940002, 172.020004, 169.5     , 175.350006,\n",
      "       179.369995, 180.919998, 182.559998, 183.740005, 185.789993,\n",
      "       185.919998, 184.729996, 189.809998, 190.779999, 186.600006,\n",
      "       184.580002, 186.550003, 187.529999, 188.539993, 188.380005,\n",
      "       191.050003, 193.149994, 193.139999, 195.929993, 196.339996,\n",
      "       199.229996, 198.179993, 198.440002, 196.210007, 198.009995,\n",
      "       198.559998, 198.610001, 202.520004, 202.339996, 203.899994,\n",
      "       207.050003, 205.119995, 202.119995, 203.860001, 199.110001,\n",
      "       209.229996, 208.130005, 210.229996, 203.5     , 200.830002,\n",
      "       201.75    , 196.660004, 192.770004, 182.850006, 185.410004,\n",
      "       186.020004, 188.839996, 186.759995, 180.279999, 184.699997,\n",
      "       182.550003, 177.809998, 178.619995, 177.910004, 176.      ,\n",
      "       176.669998, 174.990005, 170.270004, 174.520004, 181.139999,\n",
      "       182.149994, 185.770004, 191.619995, 193.600006, 193.389999,\n",
      "       193.600006, 190.300003, 192.169998, 195.210007, 197.309998,\n",
      "       198.029999, 198.149994, 198.169998, 195.289993, 197.350006,\n",
      "       199.570007, 197.050003, 200.649994, 201.360001, 202.690002,\n",
      "       202.899994, 198.410004, 198.809998, 201.559998, 201.710007,\n",
      "       202.199997, 204.      , 203.5     , 203.270004, 203.699997,\n",
      "       202.360001, 203.610001, 207.289993, 207.169998, 206.729996,\n",
      "       207.139999, 208.440002, 207.309998, 211.300003, 206.740005,\n",
      "       201.630005, 192.580002, 194.039993, 193.820007, 199.389999,\n",
      "       199.289993, 199.149994, 200.479996, 202.589996, 199.669998,\n",
      "       203.839996, 210.029999, 210.320007, 211.600006, 210.75    ,\n",
      "       201.      , 205.059998, 203.529999, 203.320007, 206.660004,\n",
      "       207.199997, 204.220001, 207.320007, 211.509995, 212.509995,\n",
      "       211.070007, 211.710007, 217.729996, 222.860001, 217.020004,\n",
      "       217.559998, 219.119995, 219.440002, 220.369995, 217.470001,\n",
      "       217.649994, 217.190002, 217.139999, 218.830002, 217.279999,\n",
      "       220.789993, 224.199997, 217.929993, 215.130005, 223.889999,\n",
      "       225.839996, 225.089905])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "LowHigh = np.loadtxt('AAPL.csv', delimiter=',', dtype='float,float', usecols=(2, 3), skiprows=1, unpack=True)\n",
    "\n",
    "print(LowHigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Use the max and min functions to get the highest high and the lowest low value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min and Max High Price => (145.720001, 229.929993)\n"
     ]
    }
   ],
   "source": [
    "minHighPrice = np.min(LowHigh[0])\n",
    "\n",
    "maxHighPrice = np.max(LowHigh[0])\n",
    "\n",
    "print(f\"Min and Max High Price => {minHighPrice, maxHighPrice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min and Max Low Price => (142.0, 225.839996)\n"
     ]
    }
   ],
   "source": [
    "minLowPrice = np.min(LowHigh[1])\n",
    "\n",
    "maxLowPrice = np.max(LowHigh[1])\n",
    "\n",
    "print(f\"Min and Max Low Price => {minLowPrice, maxLowPrice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Load data from column 5 of AAPL.csv. Also, load data from column 5 of MSFT.csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[223.770004 226.869995 216.360001 214.449997 222.110001 217.360001\n",
      " 222.149994 221.190002 216.020004 219.309998 220.649994 222.729996\n",
      " 215.089996 219.800003 216.300003 212.240005 213.300003 218.860001\n",
      " 222.220001 207.479996 201.589996 203.770004 209.949997 208.490005\n",
      " 204.470001 194.169998 192.229996 186.800003 191.410004 193.529999\n",
      " 185.860001 176.979996 176.779999 172.289993 174.619995 174.240005\n",
      " 180.940002 179.550003 178.580002 184.820007 176.690002 174.720001\n",
      " 168.490005 169.600006 168.630005 169.100006 170.949997 165.479996\n",
      " 163.940002 166.070007 160.889999 156.830002 150.729996 146.830002\n",
      " 157.169998 156.149994 156.229996 157.740005 157.919998 142.190002\n",
      " 148.259995 147.929993 150.75     153.309998 153.800003 152.289993\n",
      " 150.       153.070007 154.940002 155.860001 156.820007 153.300003\n",
      " 153.919998 152.699997 157.759995 156.300003 154.679993 165.25\n",
      " 166.440002 166.520004 171.25     174.179993 174.240005 170.940002\n",
      " 170.410004 169.429993 170.889999 170.179993 170.800003 170.419998\n",
      " 170.929993 172.029999 171.059998 172.970001 174.229996 174.330002\n",
      " 174.869995 173.149994 174.970001 175.850006 175.529999 174.520004\n",
      " 172.5      172.910004 178.899994 180.910004 181.710007 183.729996\n",
      " 186.119995 188.020004 186.529999 188.160004 195.089996 191.050003\n",
      " 188.740005 186.789993 188.470001 188.720001 189.949997 191.240005\n",
      " 194.020004 195.350006 195.690002 197.       200.100006 199.5\n",
      " 200.619995 198.949997 198.869995 199.229996 199.25     203.130005\n",
      " 203.860001 204.529999 207.479996 207.160004 205.279999 204.300003\n",
      " 204.610001 200.669998 210.520004 209.149994 211.75     208.479996\n",
      " 202.860001 202.899994 200.720001 197.179993 185.720001 188.660004\n",
      " 190.919998 190.080002 189.       183.089996 186.600006 182.779999\n",
      " 179.660004 178.970001 178.229996 177.380005 178.300003 175.070007\n",
      " 173.300003 179.639999 182.539993 185.220001 190.149994 192.580002\n",
      " 194.809998 194.190002 194.149994 192.740005 193.889999 198.449997\n",
      " 197.869995 199.460007 198.779999 198.580002 195.570007 199.800003\n",
      " 199.740005 197.919998 201.550003 202.729996 204.410004 204.229996\n",
      " 200.020004 201.240005 203.229996 201.75     203.300003 205.210007\n",
      " 204.5      203.350006 205.660004 202.589996 207.220001 208.839996\n",
      " 208.669998 207.020004 207.740005 209.679993 208.779999 213.039993\n",
      " 208.429993 204.020004 193.339996 197.       199.039993 203.429993\n",
      " 200.990005 200.479996 208.970001 202.75     201.740005 206.5\n",
      " 210.350006 210.360001 212.639999 212.460007 202.639999 206.490005\n",
      " 204.160004 205.529999 209.009995 208.740005 205.699997 209.190002\n",
      " 213.279999 213.259995 214.169998 216.699997 223.589996 223.089996\n",
      " 218.75     219.899994 220.699997 222.770004 220.960007 217.729996\n",
      " 218.720001 217.679993 221.029999 219.889999 218.820007 223.970001\n",
      " 224.589996 218.960007 220.820007 227.009995 227.059998 225.809998]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "aaplCol5 = np.loadtxt('AAPL.csv', delimiter=',', dtype='float', usecols=(4), skiprows=1, unpack=True)\n",
    "\n",
    "msftCol5 = np.loadtxt('MSFT.csv', delimiter=',', dtype='float', usecols=(4), skiprows=1, unpack=True)\n",
    "\n",
    "print(aaplCol5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110.849998 112.260002 106.160004 105.910004 109.57     107.599998\n",
      " 111.       110.709999 108.5      108.660004 109.629997 108.099998\n",
      " 102.32     108.300003 106.959999 103.849998 103.730003 106.809998\n",
      " 105.919998 106.160004 107.510002 107.720001 111.959999 111.75\n",
      " 109.57     106.870003 106.940002 104.970001 107.279999 108.290001\n",
      " 104.620003 101.709999 103.110001 103.07     106.470001 107.139999\n",
      " 111.120003 110.190002 110.889999 112.089996 108.519997 109.190002\n",
      " 104.82     107.589996 108.589996 109.080002 109.449997 106.029999\n",
      " 102.889999 103.970001 103.690002 101.510002  98.230003  94.129997\n",
      " 100.559998 101.18     100.389999 101.57     101.120003  97.400002\n",
      " 101.93     102.059998 102.800003 104.269997 103.599998 102.800003\n",
      " 102.050003 105.010002 105.379997 106.120003 107.709999 105.68\n",
      " 106.709999 106.199997 107.169998 105.080002 102.940002 106.379997\n",
      " 104.43     102.779999 105.739998 107.220001 106.029999 105.269997\n",
      " 105.669998 105.25     106.889999 106.809998 106.900002 108.220001\n",
      " 108.169998 107.150002 109.410004 110.970001 111.589996 112.360001\n",
      " 112.169998 112.029999 112.529999 112.260002 111.699997 111.75\n",
      " 110.389999 110.510002 112.830002 113.620003 114.5      114.589996\n",
      " 115.910004 117.57     117.650002 117.519997 120.220001 117.050003\n",
      " 117.660004 117.910004 116.769997 116.93     117.940002 119.019997\n",
      " 119.190002 119.970001 119.360001 119.889999 119.93     119.279999\n",
      " 120.190002 120.330002 120.949997 121.050003 120.769997 121.769997\n",
      " 123.370003 123.760002 125.440002 125.010002 129.149994 129.889999\n",
      " 129.770004 130.600006 127.879997 126.209999 128.899994 128.149994\n",
      " 125.519997 125.510002 125.5      127.129997 123.349998 124.730003\n",
      " 126.019997 128.929993 128.070007 126.220001 126.900002 127.669998\n",
      " 126.18     126.239998 126.160004 124.940002 125.730003 123.68\n",
      " 119.839996 123.160004 125.830002 127.82     131.399994 132.600006\n",
      " 132.100006 131.490005 132.320007 132.449997 132.850006 135.160004\n",
      " 135.690002 136.949997 136.970001 137.779999 133.429993 133.929993\n",
      " 134.149994 133.960007 135.679993 136.580002 137.460007 137.059998\n",
      " 136.960007 136.460007 137.850006 138.399994 138.899994 138.899994\n",
      " 137.080002 136.270004 136.419998 136.619995 138.429993 139.289993\n",
      " 140.720001 140.190002 141.339996 141.029999 140.350006 136.270004\n",
      " 138.059998 136.899994 132.210007 134.690002 135.279999 138.889999\n",
      " 137.710007 135.789993 138.600006 133.979996 133.679993 136.130005\n",
      " 138.410004 137.259995 138.789993 137.779999 133.389999 135.449997\n",
      " 135.740005 135.559998 138.119995 137.860001 136.039993 137.630005\n",
      " 140.050003 139.100006 137.520004 136.080002 136.119995 137.520004\n",
      " 137.320007 136.330002 137.389999 138.520004 141.070007 139.440002\n",
      " 139.139999 137.380005 139.360001 139.539993 137.729996 139.029999\n",
      " 137.070007 134.649994 136.279999 138.119995 137.119995 136.429993]\n"
     ]
    }
   ],
   "source": [
    "print(msftCol5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Calculate the covariance matrix of the closing prices of AAPL and MSFT (hint: use the cov function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[434.60508015 186.97834783]\n",
      " [186.97834783 184.73421932]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "covMatrix = np.cov(aaplCol5,msftCol5)\n",
    "\n",
    "print(covMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) View the values on the diagonal (hint: diagonal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The diagonal values are => [434.60508015 184.73421932]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The diagonal values are => {np.cov(aaplCol5,msftCol5).diagonal()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Calculate the correlation coefficient of the closing prices of AAPL and MSFT (hint: corrcoef)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.65988802]\n",
      " [0.65988802 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "corrCoefficient = np.corrcoef(aaplCol5,msftCol5)\n",
    "\n",
    "print(corrCoefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expresssions Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Write a Python program that will identify URLs using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://google.com', 'http://google.com']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regex = r\"http[s]?:\\/\\/\\w+\\.com?\"\n",
    "testString = \"https://google.comhttp://google.comhtts://google.comhttp:/google.com\"\n",
    "x = re.findall(regex, testString)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urls:  ['https://auth.geeksforgeeks.org/user/Chinmoy%20Lenka/articles', 'http://www.geeksforgeeks.org/']\n"
     ]
    }
   ],
   "source": [
    "# Python code to find the URL from an input string \n",
    "# Using the regular expression \n",
    "import re \n",
    "  \n",
    "def Find(string): \n",
    "  \n",
    "    # findall() has been used  \n",
    "    # with valid conditions for urls in string \n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    url = re.findall(regex,string)       \n",
    "    return [x[0] for x in url] \n",
    "      \n",
    "# Driver Code \n",
    "string = 'My Profile: https://auth.geeksforgeeks.org/user/Chinmoy%20Lenka/articles in the portal of http://www.geeksforgeeks.org/'\n",
    "print(\"Urls: \", Find(string)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analytics Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Complete the tutorial at https://data-flair.training/blogs/nltk-python-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To install NLTK, you can use Python pip-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\kenec\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\kenec\\anaconda3\\lib\\site-packages (from nltk) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then to import it, you can type in the interpreter-\n",
    "### You must know about Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, to install packages from NLTK, you need to use its downloader. Try this-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kenec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kenec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kenec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kenec\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Python Tutorial – NLTK Tokenize Text\n",
    "\n",
    "### a) NLTK Sentence Tokenizer - Let’s try tokenizing a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today is a great day.',\n",
       " 'It is even better than yesterday.',\n",
       " 'And yesterday was the best day ever.']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Today is a great day. It is even better than yesterday. And yesterday was the best day ever.\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi, how are you?', \"I'm good, you?\", 'Great!']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"Hi, how are you? I'm good, you? Great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Last night, I went to Mrs. Martinez's housewarming.\", 'It was a disaster.']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(\"Last night, I went to Mrs. Martinez's housewarming. It was a disaster.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Enchanté, comment allez-vous?', 'Tres bien.', 'Mersi, et vous?']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"Enchanté, comment allez-vous? Tres bien. Mersi, et vous?\",\"french\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues while tokenizing- One issue we face while tokenizing is abbreviations-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She holds an MDS.', 'in Oral Pathology']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"She holds an MDS. in Oral Pathology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) NLTK Word Tokenizer - First, let’s tokenize text in NLTK Python Tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'day',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'even',\n",
       " 'better',\n",
       " 'than',\n",
       " 'yesterday',\n",
       " '.',\n",
       " 'And',\n",
       " 'yesterday',\n",
       " 'was',\n",
       " 'the',\n",
       " 'best',\n",
       " 'day',\n",
       " 'ever',\n",
       " '.']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Synonyms From NLTK WordNet - WordNet is an NLP database with synonyms, antonyms, and brief definitions. We downloaded this with the NLTK downloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('love.n.01'),\n",
       " Synset('love.n.02'),\n",
       " Synset('beloved.n.01'),\n",
       " Synset('love.n.04'),\n",
       " Synset('love.n.05'),\n",
       " Synset('sexual_love.n.02'),\n",
       " Synset('love.v.01'),\n",
       " Synset('love.v.02'),\n",
       " Synset('love.v.03'),\n",
       " Synset('sleep_together.v.01')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn=wordnet.synsets('love')\n",
    "syn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let’s choose the first member from this-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a strong positive emotion of regard and affection'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['his love for his work', 'children need a lot of love']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn[0].examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But love isn’t the only thing we’re chasing after. Do you know the meaning of life? 42?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a characteristic state or mode of living'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn=wordnet.synsets('life')\n",
    "syn[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['social life', 'city life', 'real life']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn[0].examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nope, still doesn’t answer our questions. But Python does a good job. Okay, one more example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('army_intelligence.n.01'),\n",
       " Synset('artificial_intelligence.n.01'),\n",
       " Synset('three-toed_sloth.n.01'),\n",
       " Synset('artificial_insemination.n.01')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn=wordnet.synsets('AI')\n",
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the branch of computer science that deal with writing computer programs that can solve problems creatively'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn[1].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['workers in AI hope to imitate or duplicate intelligence in computers and robots']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn[1].examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get the list of synonyms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Army_Intelligence',\n",
       " 'AI',\n",
       " 'artificial_intelligence',\n",
       " 'AI',\n",
       " 'three-toed_sloth',\n",
       " 'ai',\n",
       " 'Bradypus_tridactylus',\n",
       " 'artificial_insemination',\n",
       " 'AI']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms=[]\n",
    "for syn in wordnet.synsets('AI'):\n",
    "        for lemma in syn.lemmas():\n",
    "               synonyms.append(lemma.name())\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Antonyms From NLTK WordNet - To get the list of antonyms, we first need to check the lemmas- are there antonyms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elate']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "antonyms=[]\n",
    "for syn in wordnet.synsets('depressed'):\n",
    "        for l in syn.lemmas():\n",
    "                 if l.antonyms():\n",
    "                          antonyms.append(l.antonyms()[0].name())\n",
    "antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elate', 'beautiful', 'beautiful']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for syn in wordnet.synsets('ugly'):\n",
    "        for l in syn.lemmas():\n",
    "                  if l.antonyms():\n",
    "                            antonyms.append(l.antonyms()[0].name())\n",
    "antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Python Tutorial – Stemming NLTK\n",
    "\n",
    "### Stemming involves removing affixes from words and returning the root. Search engines like Google use this to efficiently index pages. The most common algorithm for stemming is the PorterStemmer. Let’s take an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmer.stem('loving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'traine'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('trainee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'syllabi'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('syllabi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alibi'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('alibi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'formula'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('formulae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'criteria'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('criteria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believ'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('believes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('writes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('writing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('write')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming Words from Other Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'englez'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rom_stemmer=SnowballStemmer('romanian')\n",
    "rom_stemmer.stem('englezească') #English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cocoș'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rom_stemmer.stem('cocoș') #Cocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frigid'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rom_stemmer.stem('frigidere') #Refrigerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frigider'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rom_stemmer.stem('frigider') #Refrigerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goi'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rom_stemmer.stem('goi') #Empty- plural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goal'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rom_stemmer.stem('goale') #Empty- plural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frumoas'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rom_stemmer.stem('frumoasă') #Beautiful- female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frumoas'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rom_stemmer.stem('frumoase') #Beautiful- plural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frumoș'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rom_stemmer.stem('frumoși') #Beautiful- plural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python NLTK Tutorial – Lemmatizing NLTK Using WordNet\n",
    "#### If you noticed, some words that stemming gave us weren’t actual words you could look up in the dictionary. So we come to lemmatizing- this will return real words. Let’s do this too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'belief'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('believes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believe'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('believes',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crossing'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('crossing',pos='a') #adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cross'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('crossing',pos='v') #verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crossing'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('crossing',pos='n') #noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crossing'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('crossing',pos='r') #adverb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python NLTK Tutorial – NLTK Stop Words - We can filter NLTK stop words from text before processing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today',\n",
       " 'great',\n",
       " 'day',\n",
       " '.',\n",
       " 'It',\n",
       " 'even',\n",
       " 'better',\n",
       " 'yesterday',\n",
       " '.',\n",
       " 'And',\n",
       " 'yesterday',\n",
       " 'best',\n",
       " 'day',\n",
       " 'ever',\n",
       " '!']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "text=\"Today is a great day. It is even better than yesterday. And yesterday was the best day ever!\"\n",
    "stopwords=set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "words=word_tokenize(text)\n",
    "wordsFiltered=[]\n",
    "for w in words:\n",
    "        if w not in stopwords:\n",
    "                 wordsFiltered.append(w)\n",
    "wordsFiltered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Python Tutorial – Speech Tagging - NLTK can classify words as verbs, nouns, adjectives, and more into one of the following classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('human', 'JJ'), ('being', 'VBG'), (',', ','), ('capable', 'JJ'), ('of', 'IN'), ('doing', 'VBG'), ('terrible', 'JJ'), ('things', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "text='I am a human being, capable of doing terrible things'\n",
    "sentences=nltk.sent_tokenize(text)\n",
    "for sent in sentences:\n",
    "        print(nltk.pos_tag(nltk.word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Python | Check for URL in a String | GeeksforGeeks - https://www.geeksforgeeks.org/python-check-url-string/\n",
    "\n",
    "### NLTK Python Tutorial (Natural Language Toolkit) | DataFair - https://data-flair.training/blogs/nltk-python-tutorial/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
